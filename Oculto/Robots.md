Existem vários locais que podemos verificar manualmente em um site para começar a descobrir mais conteúdo. 

Robots.txt

O arquivo robots.txt é um documento que informa aos mecanismos de busca quais páginas eles podem ou não exibir nos resultados de pesquisa, ou ainda que proíbe mecanismos de busca específicos de rastrearem o site por completo. É prática comum restringir certas áreas de um site para que não sejam exibidas nos resultados de busca. Essas páginas podem ser áreas como portais de administração ou arquivos destinados aos clientes do site. Esse arquivo nos fornece uma lista valiosa de locais no site que os proprietários não querem que nós, como testadores de penetração, descubramos.
